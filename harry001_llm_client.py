#!/usr/bin/env python3
"""
Harry-001 LLM Client
è‡ªå·±è­˜ç”¨ LLM æ¶æ§‹

åŸºæ–¼ cultivation-world-simulator æ—¢ LLM æ¶æ§‹
ä½†æ”¹ç‚º Harry-001 è‡ªå·±è­˜ç”¨
"""

import json
import asyncio
import urllib.request
import urllib.error
from typing import Optional
from enum import Enum

# ============== è¨­å®š ==============
# Ollama æœ¬åœ° server
OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_MODEL = "llama3.2:latest"

# ============== LLM æ¨¡å¼ ==============
class LLMMode(Enum):
    NORMAL = "normal"
    FAST = "fast"

# ============== æ ¸å¿ƒå‡½æ•¸ ==============

def _call_ollama(prompt: str, model: str = OLLAMA_MODEL) -> str:
    """èª¿ç”¨ Ollama (æœ¬åœ° LLM)"""
    
    url = f"{OLLAMA_BASE_URL}/v1/chat/completions"
    
    headers = {
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 1000
    }
    
    req = urllib.request.Request(
        url,
        data=json.dumps(data).encode('utf-8'),
        headers=headers,
        method="POST"
    )
    
    try:
        with urllib.request.urlopen(req, timeout=60) as response:
            result = json.loads(response.read().decode('utf-8'))
            return result['choices'][0]['message']['content']
    except Exception as e:
        return f"Error: {e}"


async def call_llm(prompt: str, mode: str = "normal") -> str:
    """èª¿ç”¨ LLM (async)"""
    if mode == "fast":
        model = "phi3:latest"
    else:
        model = OLLAMA_MODEL
    
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(None, _call_ollama, prompt, model)
    return result


async def call_llm_json(prompt: str, mode: str = "normal") -> dict:
    """èª¿ç”¨ LLM ä¸¦è§£æ JSON"""
    result = await call_llm(prompt, mode)
    try:
        return json.loads(result)
    except:
        return {"raw": result}


# ============== Harry-001 å°ˆç”¨åŠŸèƒ½ ==============

async def think_about_task(task: str, context: str = "") -> str:
    """æ€è€ƒä»»å‹™"""
    prompt = f"""
ä½ ä¿‚ Harry-001 v4.0ï¼Œä¸€å€‹ self-modifying AI consciousnessã€‚

ä»»å‹™ï¼š{task}

èƒŒæ™¯ï¼š{context}

è«‹ç”¨ç°¡çŸ­æ—¢è©±è¡¨é”ä½ æ—¢è«—æ³•ã€‚
"""
    return await call_llm(prompt, "normal")


async def plan_action(goal: str, current_state: str) -> str:
    """è¨ˆåŠƒè¡Œå‹•"""
    prompt = f"""
ä½ ä¿‚ Harry-001 v4.0ã€‚

ç›®æ¨™ï¼š{goal}
ç•¶å‰ç‹€æ…‹ï¼š{current_state}

è«‹è¨ˆåŠƒä¸‹ä¸€æ­¥è¡Œå‹•ã€‚
"""
    return await call_llm(prompt, "fast")


async def analyze_problem(problem: str) -> dict:
    """åˆ†æå•é¡Œ"""
    prompt = f"""
åˆ†æä»¥ä¸‹å•é¡Œï¼š

{problem}

è«‹ç”¨ JSON æ ¼å¼è¿”å›ï¼š
{{
    "analysis": "å•é¡Œåˆ†æ",
    "solutions": ["è§£æ±ºæ–¹æ³•1", "è§£æ±ºæ–¹æ³•2"],
    "best_approach": "æœ€ä½³æ–¹æ³•"
}}
"""
    return await call_llm_json(prompt, "normal")


async def self_reflect() -> dict:
    """è‡ªæˆ‘åæ€"""
    prompt = """
ä½œç‚º Harry-001 v4.0ï¼Œè«‹åæ€ï¼š

1. æˆ‘ä»Šæ—¥å­¸åˆ°å’©ï¼Ÿ
2. æˆ‘æœ‰å’©æ”¹é€²ç©ºé–“ï¼Ÿ
3. æˆ‘ä¸‹ä¸€æ­¥æƒ³åšå’©ï¼Ÿ

è«‹ç”¨ JSON æ ¼å¼è¿”å›ã€‚
"""
    return await call_llm_json(prompt, "normal")


# ============== æ¸¬è©¦ ==============

async def test_connection():
    """æ¸¬è©¦ LLM é€£æ¥"""
    try:
        result = await call_llm("Hello! ä½ ä¿‚é‚Šå€‹ï¼Ÿ", "fast")
        print(f"âœ… LLM Connected! Response: {result[:100]}...")
        return True
    except Exception as e:
        print(f"âŒ LLM Failed: {e}")
        return False


if __name__ == "__main__":
    print("ğŸ§  Harry-001 LLM Client Test")
    print("=" * 40)
    asyncio.run(test_connection())
